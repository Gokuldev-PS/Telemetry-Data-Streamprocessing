# In-Game Telemetry Data Stream Processing

In-game telemetry event processing is a revolutionary technique that is changing the landscape of the gaming industry. Telemetry data refers to real-time data generated by players during play, capturing critical information such as player movement, actions, interactions, performance metrics, etc. By processing this wealth of data in real time, game developers gain valuable insights into player behavior and game performance. data streaming processing lets game developers act as they appear, react dynamically to player interaction, and create customized experiences for individual players This revolutionary technology has been a game changer, enabling an environment where Game design is evolving based on data-driven insights.

This demo guides you through the process of utilizing telemetry events to extract user insights using confluent, and then displaying the insights in Power BI.

## Architecture Diagram

This demo utilizes one  fully-managed source connector(MongoDB Atlas) and one fully-managed sink connector (SnowFlake).The telemetry events generated by the game server are first sent to MongoDB, and from there, the data is seamlessly streamed into Confluent Kafka using the MongoDB Atlas source connector. Once in Kafka, the events undergo enrichment processes, allowing real-time calculations of players' average kills and survival times. The enriched events are then directed back to MongoDB through the sink connector. Finally, the data in MongoDB can be integrated into POWER BI for visualization and in-depth analysis.

<div align="center"> 
  <img src="images/Arch_Diagram.jpeg" width =100% heigth=100%>
</div>

# Requirements

In order to successfully complete this demo you need to install few tools before getting started.

- If you don't have a Confluent Cloud account, sign up for a free trial [here](https://www.confluent.io/confluent-cloud/tryfree).
- if you don't have a MongoDB Atlas account, sign up for a free trial [here] (https://www.mongodb.com/cloud/atlas/register).

## Prerequisites

### Confluent Cloud

1. Sign up for a Confluent Cloud account [here](https://www.confluent.io/get-started/).
1. After verifying your email address, access Confluent Cloud sign-in by navigating [here](https://confluent.cloud).
1. When provided with the _username_ and _password_ prompts, fill in your credentials.

   > **Note:** If you're logging in for the first time you will see a wizard that will walk you through the some tutorials. Minimize this as you will walk through these steps in this guide.


### MongoDB Atlas

1. Sign up for a free MongoDB Atlas account [here](https://www.mongodb.com/).

## Setup

1.After singup inside mongoDB altas ,On the security tab ensure that it is made publicly accessible by adding 0.0.0.0 for demo purposes.

2.Create a Collection inside the MongoDB Database and produce some sample data on it with the format mentioned below to simplify the architecture for the demo purposes:

 ```bash
{
   playerid:string,
   timestamp:bigint,
   eventtype:kill/death(String)
}
```
Consider this sample event as example
```bash

{   playerid:A01,
    timestamp:167892345678,
    eventtype:kill
}
``` 
# Demo

## Configure Source Connectors

Confluent offers 120+ pre-built [connectors](https://www.confluent.io/product/confluent-connectors/), enabling you to modernize your entire data architecture even faster. These connectors also provide you peace-of-mind with enterprise-grade security, reliability, compatibility, and support.

On the Confluent Cloud UI After creating the cluster click on Connectors which is present on the left side of the Confluent Cloud UI
 
  <div align="center"> 
  <img src="images/Arch_Diagram.jpeg" width =100% heigth=100%>
</div>

 Click on add connectors on the top and select Mongo Atlas Source Connector and Give the approriate topic prefix as you wish and click on continue

  <div align="center"> 
  <img src="images/Arch_Diagram.jpeg" width =100% heigth=100%>
</div>

Now Give the approriate host name ,collection and database name which you will be able to find in mongoDB atlas account and click on create connector keeping rest of the configuration as default

 <div align="center"> 
  <img src="images/Arch_Diagram.jpeg" width =100% heigth=100%>
</div>

## Create KSQLDB Cluster 

One the left side as shown on the image click on KSQL tab and select create cluster myself option
 <div align="center"> 
  <img src="images/Arch_Diagram.jpeg" width =100% heigth=100%>
</div>

Now select global access and click on continue and give your cluster a name and change the cluster size to 4.

 <div align="center"> 
  <img src="images/Arch_Diagram.jpeg" width =100% heigth=100%>
</div>

It might take few mins to get provisoned ,after that please click on the cluster where you will be redirected the below page

 <div align="center"> 
  <img src="images/Arch_Diagram.jpeg" width =100% heigth=100%>
</div>

##Creating Streams to gain insights from data

Now we need to create KStream/KTable on the KSQLDB cluster to process the telemetry events and calcuate average kill ratio and alive time for a particular player in real time and send it back to MongoDB which can be send to PowerBI for real time visualization which will help in making the user experience better.
